import streamlit as st
import pandas as pd
import numpy as np
import os
import joblib
import re
import json
from groq import Groq # Importar la librer칤a Groq
import requests # Importar la librer칤a requests para hacer llamadas HTTP a Flask

# --- 1. Constantes y Configuraci칩n de Rutas ---
# Rutas a los directorios de modelos y reportes
MODELS_DIR = os.path.join(os.path.dirname(__file__), '..', 'models')
REPORTS_DIR = os.path.join(os.path.dirname(__file__), '..', 'reports')

# URL de tu backend Flask para guardar datos
# IMPORTANTE: Si despliegas Flask en un servidor diferente, esta URL deber치 cambiar
# Para Docker Compose, usa el nombre del servicio Flask: http://flask_backend:5000/save_company_data
FLASK_BACKEND_SAVE_URL = "http://flask_backend:5000/save_company_data" 

# Caracter칤sticas importantes para mostrar en el perfil del cl칰ster
DISPLAY_FEATURES_FOR_CLUSTERS = [
    'roaa_before_interest_and_percent_after_tax',
    'operating_gross_margin',
    'cash_flow_rate',
    'debt_ratio_percent',
    'equity_to_liability',
    'working_capital_to_total_assets',
    'current_liability_to_assets',
    'total_asset_growth_rate',
    'retained_earnings_to_total_assets'
]

# --- 2. Carga Segura de la Clave API de Groq ---
# Accede a la clave API de Groq de forma segura usando st.secrets
# Esto requiere un archivo .streamlit/secrets.toml con [groq] api_key = "tu_clave"
try:
    GROQ_API_KEY_VALUE = st.secrets["groq"]["api_key"]
    client = Groq(api_key=GROQ_API_KEY_VALUE)
except KeyError:
    st.error("Error: La clave API de Groq no se encontr칩 en st.secrets. Aseg칰rate de configurar .streamlit/secrets.toml.")
    st.stop()
except Exception as e:
    st.error(f"Error al inicializar Groq API. Detalles: {e}")
    st.stop()


# --- 3. Funciones de Carga de Objetos (Cacheadas para Rendimiento) ---
@st.cache_resource
def load_object(file_path: str):
    """
    Carga un objeto usando joblib.
    """
    try:
        obj = joblib.load(file_path)
        return obj
    except FileNotFoundError:
        st.error(f"Error: Archivo no encontrado en '{file_path}'. Por favor, aseg칰rate de que el archivo exista y la ruta sea correcta.")
        return None
    except Exception as e:
        st.error(f"Error al cargar el objeto desde {file_path}: {e}")
        return None

@st.cache_data
def load_json_data(file_path: str):
    """
    Carga datos desde un archivo JSON.
    """
    try:
        with open(file_path, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        st.warning(f"Advertencia: Archivo JSON no encontrado en '{file_path}'.")
        return {}
    except json.JSONDecodeError:
        st.warning(f"Advertencia: Error al decodificar el archivo JSON en '{file_path}'. Podr칤a estar corrupto o vac칤o.")
        return {}
    except Exception as e:
        st.warning(f"Error inesperado al cargar el JSON desde {file_path}: {e}")
        return {}


# --- 4. Funciones de Preprocesamiento (Replicando la l칩gica de data_processing.py) ---

def limpiar_nombres_columnas(df: pd.DataFrame) -> pd.DataFrame:
    """
    Limpieza de los nombres de las columnas de un DataFrame.
    """
    new_columns = []
    for col in df.columns:
        col = str(col).strip()
        col = col.lower()
        col = col.replace(' ', '_')
        col = col.replace('%', 'percent')
        col = col.replace('(', '')
        col = col.replace(')', '')
        col = col.replace('-', '_')
        col = col.replace('/', '_')
        col = re.sub(r'[^a-z0-9_]', '', col)
        col = col.replace('___', '_').replace('__', '_') # Reducir m칰ltiples underscores
        col = col.strip('_') # Eliminar underscores al principio/final
        new_columns.append(col)
    df.columns = new_columns
    return df

def apply_full_preprocessing_pipeline(df_raw: pd.DataFrame, preprocessor_params: dict, scaler: object, expected_features_for_scaler: list) -> pd.DataFrame:
    """
    Aplica el pipeline de preprocesamiento completo a un DataFrame de entrada.
    """
    df_processed = df_raw.copy()

    # 1. Limpiar nombres de columnas
    df_processed = limpiar_nombres_columnas(df_processed)

    # 2. Convertir a num칠rico, manejar infinitos
    for col in df_processed.columns:
        df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')
    df_processed.replace([np.inf, -np.inf], np.nan, inplace=True)

    # 3. Eliminar 'net_income_flag' si existe
    if 'net_income_flag' in df_processed.columns:
        df_processed.drop(columns=['net_income_flag'], inplace=True)

    # 4. Manejar valores at칤picos con clipping
    clipping_bounds = preprocessor_params.get('clipping_bounds', {})
    for col, bounds in clipping_bounds.items():
        if col in df_processed.columns and df_processed[col].dtype in ['float64', 'int64']:
            df_processed[col] = np.clip(df_processed[col], bounds['lower'], bounds['upper'])

    # 5. Imputar valores faltantes con la mediana
    imputation_means = preprocessor_params.get('imputation_means', {})
    for col, mean_val in imputation_means.items():
        if col in df_processed.columns and df_processed[col].isnull().any():
            df_processed[col].fillna(mean_val, inplace=True)
    
    # 6. Eliminar caracter칤sticas altamente correlacionadas
    cols_to_drop_corr = preprocessor_params.get('columns_dropped_correlation', [])
    for col in cols_to_drop_corr:
        if col in df_processed.columns:
            df_processed.drop(columns=[col], inplace=True)

    # 7. Asegurar que el DataFrame tiene las columnas esperadas por el scaler y en el orden correcto
    df_final_features = df_processed.reindex(columns=expected_features_for_scaler, fill_value=0.0)

    # Verificaci칩n final de NaNs antes de escalar
    if df_final_features.isnull().sum().sum() > 0:
        st.warning("춰Advertencia! Se encontraron valores nulos despu칠s del preprocesamiento. Imputando con 0.0 para evitar errores en el escalador.")
        df_final_features.fillna(0.0, inplace=True)

    # 8. Aplicar el escalador
    try:
        scaled_array = scaler.transform(df_final_features)
        df_scaled = pd.DataFrame(scaled_array, columns=expected_features_for_scaler)
    except Exception as e:
        st.error(f"Error al aplicar el escalado a los datos de entrada: {e}")
        st.warning("Verifica que las columnas de entrada coincidan con las esperadas por el escalador.")
        st.write("Columnas esperadas por el escalador:", expected_features_for_scaler)
        st.write("Columnas en la entrada despu칠s de preprocesamiento:", df_final_features.columns.tolist())
        st.stop() # Detener la ejecuci칩n si el escalado falla

    return df_scaled


# --- 5. L칩gica de la Aplicaci칩n (Predicci칩n y An치lisis) ---

# Definici칩n de los SYSTEM PROMPTS para la IA (tal como los enviaste)
SYSTEM_PROMPT_INITIAL_ANALYSIS = "Eres un experto en finanzas, econom칤a y negocios. Tu funci칩n es analizar informaci칩n financiera y de mercado. Solo puedes responder a preguntas relacionadas con estos temas y analizar la informaci칩n brindada."
SYSTEM_PROMPT_CHAT = "Eres un experto en finanzas, econom칤a y negocios. Tu funci칩n es analizar informaci칩n financiera y de mercado. Solo puedes responder a preguntas relacionadas con estos temas.Puedes responder algunas preguntas sobre el proyecto ml, sobre el m칠todo de predicci칩n y funcionalidad b치sica del modelo, no debes entrar en detalles t칠cnicos de programaci칩n porque no es tu especialidad, los temas m치s especializados de programaci칩n se deben consultar al desarrollador. Si te preguntan sobre cualquier otro tema, debes responder: 'Lo siento, soy un experto en finanzas y no puedo ayudarte con ese tema. '"


def get_llm_response(messages: list, system_prompt: str): # A침adido system_prompt como argumento
    """
    Llama a la API de Groq para obtener una respuesta de texto.
    """
    try:
        # Asegurarse de que el system_prompt se inserta al principio de los mensajes
        full_messages = [{"role": "system", "content": system_prompt}] + messages
        
        chat_completion = client.chat.completions.create(
            messages=full_messages, # Usar full_messages
            model="llama3-8b-8192", # Puedes cambiar a "llama3-70b-8192" para un modelo m치s potente
            stream=False,
            temperature=0.7, # Controla la creatividad (0.0 a 2.0)
            max_tokens=700 # Limita la longitud de la respuesta
        )
        return chat_completion.choices[0].message.content
    except Exception as e:
        st.error(f"Error al comunicarse con la API de Groq: {e}")
        return "An치lisis no disponible debido a un error de comunicaci칩n con la IA."

def generate_initial_analysis_prompt_content(prediction_result, prediction_proba, cluster_id, prop_quiebra_cluster, cluster_profile_data, original_input_df_head):
    """
    Genera el contenido del prompt inicial detallado para el an치lisis de la IA.
    """
    prompt = f"""
    Aqu칤 tienes informaci칩n sobre un proyecto de Machine Learning para predecir la quiebra de empresas y los resultados de una empresa espec칤fica. El modelo fue entrenado y validado para ser robusto.

    **Contexto del Proyecto de ML:**
    - Objetivo: Predecir la quiebra empresarial minimizando falsos negativos (priorizando Recall).
    - Dataset original de entrenamiento: 6,819 registros, 95 variables financieras.
    - Preprocesamiento: Incluy칩 limpieza, manejo de NaN/outliers, eliminaci칩n de 24 caracter칤sticas correlacionadas, estandarizaci칩n (StandardScaler) a 78 caracter칤sticas finales.
    - Modelo de Clasificaci칩n Final: RandomForest (Class Weighted).
        - M칠tricas clave en el conjunto de prueba: Recall (Quiebra) = 0.659, Precision (Quiebra) = 0.246, F1-Score (Quiebra) = 0.358, ROC AUC = 0.898.
        - La elecci칩n de este modelo prioriza capturar la mayor칤a de las quiebras reales, aceptando un mayor n칰mero de falsas alarmas, ya que el costo de no detectar una quiebra es m치s alto.
    - An치lisis de Cl칰steres (KMeans sobre PCA): Identifica 3 perfiles de riesgo distintos:
        - Cl칰ster 0 (Bajo Riesgo): 0.14% de quiebras hist칩ricas. Representa empresas muy saludables.
        - Cl칰ster 1 (Riesgo Extremo): 75.00% de quiebras hist칩ricas (aunque es un cl칰ster muy peque침o, es una se침al de alarma cr칤tica). Representa empresas en crisis severa.
        - Cl칰ster 2 (Riesgo Moderado): 4.52% de quiebras hist칩ricas. Representa empresas con un perfil promedio.

    **Informaci칩n de la Empresa a Analizar:**
    - Predicci칩n del modelo: {'ALTA PROBABILIDAD de QUiebra' if prediction_result == 1 else 'BAJA PROBABILIDAD de Quiebra'}
    - Probabilidad de quiebra: {prediction_proba*100:.2f}%
    - Cl칰ster asignado: {cluster_id}
    - Proporci칩n hist칩rica de quiebras en este cl칰ster: {prop_quiebra_cluster*100:.2f}%
    - Perfil financiero t칤pico del Cl칰ster {cluster_id} (valores escalados para las caracter칤sticas clave):
    {json.dumps(cluster_profile_data, indent=2)}
    - Primeros valores del DataFrame de entrada original de la empresa (para contexto):
    {original_input_df_head.to_dict() if not original_input_df_head.empty else "No disponible"}

    Por favor, proporciona un an치lisis financiero detallado de esta empresa, interpretando su predicci칩n y su pertenencia al cl칰ster. Incluye:
    1.  Un resumen del riesgo general de la empresa y su salud financiera.
    2.  Las implicaciones financieras clave de su predicci칩n y su cl칰ster.
    3.  Recomendaciones o consideraciones clave para un analista de negocios o inversor basadas en este perfil.
    4.  Menciona si hay alguna inconsistencia notable entre la predicci칩n del modelo y el cl칰ster asignado (ej. modelo dice 'Baja Probabilidad' pero est치 en Cl칰ster 1).
    """
    return prompt


# --- 6. Carga de Artefactos al inicio de la aplicaci칩n ---
# Cargar el modelo final (RandomForest_ClassWeighted)
final_model = load_object(os.path.join(MODELS_DIR, 'best_rf_classweighted_model.pkl'))

# Cargar el scaler universal
scaler = load_object(os.path.join(MODELS_DIR, 'scaler_for_all_features.pkl'))

# Cargar las caracter칤sticas que el SCALER ESPERA (las 78 features despu칠s de correlaci칩n y net_income_flag)
scaler_expected_features = load_object(os.path.join(MODELS_DIR, 'scaler_features.pkl'))

# Cargar los par치metros del preprocesamiento (clipping_bounds, imputation_means, columns_dropped_correlation)
preprocessor_params = load_object(os.path.join(MODELS_DIR, 'preprocessor_params.pkl'))

# Cargar los modelos PCA y KMeans
pca_model = load_object(os.path.join(MODELS_DIR, 'pca_model.pkl'))
kmeans_model = load_object(os.path.join(MODELS_DIR, 'kmeans_model.pkl'))

# Cargar perfiles de cl칰steres para la interpretaci칩n
cluster_profiles = load_json_data(os.path.join(REPORTS_DIR, 'cluster_profiles.json'))
bankrupt_proportions = load_json_data(os.path.join(REPORTS_DIR, 'bankrupt_proportion_by_cluster.json'))


# Verificar que todos los recursos necesarios se cargaron correctamente
if not all([final_model, scaler, scaler_expected_features, preprocessor_params, pca_model, kmeans_model, cluster_profiles, bankrupt_proportions]):
    st.error("Error cr칤tico: No se pudieron cargar uno o m치s recursos necesarios. Aseg칰rate de que todos los scripts de entrenamiento y an치lisis se ejecutaron correctamente y los archivos `.pkl` y `.json` est치n en las rutas esperadas (`/models/` y `/reports/`).")
    st.stop()


# --- 7. Interfaz de Usuario de Streamlit ---
st.set_page_config(page_title="Predicci칩n de Quiebra Empresarial", layout="centered")

st.title("游눶 Predicci칩n de Quiebra Empresarial")
st.markdown("""
Esta aplicaci칩n predice la probabilidad de quiebra de una empresa bas치ndose en sus datos financieros.
Por favor, sube un archivo CSV con las caracter칤sticas de la empresa que deseas analizar.
**El archivo CSV debe contener una sola fila de datos para una empresa y todas las 95 columnas esperadas del dataset original.**
""")

st.subheader("Subir Datos de la Empresa")
uploaded_file = st.file_uploader("Arrastra o selecciona un archivo CSV", type="csv")

if uploaded_file is not None:
    try:
        input_df_raw = pd.read_csv(uploaded_file)
        st.write("Datos cargados exitosamente:")
        st.dataframe(input_df_raw)

        if input_df_raw.shape[0] > 1:
            st.warning("Advertencia: El archivo CSV contiene m칰ltiples filas. Solo se procesar치 la primera fila para la predicci칩n.")
            input_df_raw = input_df_raw.head(1)
        
        # Guardar una copia de la cabecera del DF original para el prompt del LLM
        original_input_df_head = input_df_raw.iloc[0, :5] # Primeros 5 valores para contexto

        st.subheader("Realizando Predicci칩n y An치lisis...")

        # --- Aplicar Pipeline de Preprocesamiento ---
        # df_scaled ahora contiene las 78 caracter칤sticas escaladas
        df_scaled = apply_full_preprocessing_pipeline(input_df_raw.copy(), preprocessor_params, scaler, scaler_expected_features)

        # Preparar para el modelo supervisado (RandomForest_ClassWeighted)
        # Asegurarse de que las columnas est칠n en el orden correcto para el modelo
        processed_input_for_rf = df_scaled.reindex(columns=load_object(os.path.join(MODELS_DIR, 'model_features.pkl')), fill_value=0.0)


        # --- Realizar Predicciones y An치lisis ---

        # 1. Predicci칩n de Clasificaci칩n (RandomForest_ClassWeighted)
        prediction = None
        prediction_proba = None
        try:
            prediction = final_model.predict(processed_input_for_rf)[0]
            prediction_proba = final_model.predict_proba(processed_input_for_rf)[0, 1] # Probabilidad de quiebra (clase 1)

            st.subheader("Resultados de la Predicci칩n de Quiebra (RandomForest_ClassWeighted)")
            if prediction == 1:
                st.error(f"**PREDICCI칍N: 춰La empresa tiene ALTA PROBABILIDAD de QUiebra!**")
                st.metric(label="Probabilidad de Quiebra", value=f"{prediction_proba*100:.2f}%", delta_color="inverse")
            else:
                st.success(f"**PREDICCI칍N: La empresa tiene BAJA PROBABILIDAD de Quiebra.**")
                st.metric(label="Probabilidad de Quiebra", value=f"{prediction_proba*100:.2f}%", delta_color="normal")

            st.markdown(f"*(Umbral de clasificaci칩n por defecto del modelo: > 0.5 para Quiebra)*")
        except Exception as e:
            st.error(f"Error al realizar la predicci칩n del RandomForest_ClassWeighted: {e}")
            st.warning("Aseg칰rate de que los datos de entrada est치n preparados correctamente para el modelo.")
            st.write("Columnas esperadas por RandomForest_ClassWeighted:", load_object(os.path.join(MODELS_DIR, 'model_features.pkl')))
            st.write("Columnas en entrada final para RandomForest_ClassWeighted:", processed_input_for_rf.columns.tolist())
            st.stop()


        st.write("---")
        st.subheader("An치lisis Adicional (Modelos No Supervisados)")

        # 2. An치lisis PCA
        pca_transformed_data = None
        if pca_model:
            try:
                pca_transformed_data = pca_model.transform(df_scaled)
                st.write(f"**An치lisis PCA:**")
                st.write(f"La empresa se representa en {pca_transformed_data.shape[1]} componentes principales.")
                st.write(f"Primeros componentes: {pca_transformed_data[0, :5]}")
            except Exception as e:
                st.warning(f"No se pudo aplicar PCA a los datos de entrada. Error: {e}")
                st.write("Columnas en entrada procesada y escalada para PCA:", df_scaled.columns.tolist())
                # No st.stop() aqu칤 para permitir que KMeans intente ejecutarse si PCA fall칩 por otra raz칩n

        # 3. An치lisis KMeans
        cluster_id = None
        prop_quiebra_cluster = 0
        cluster_profile_data = {}

        if kmeans_model and pca_transformed_data is not None:
            try:
                kmeans_cluster = kmeans_model.predict(pca_transformed_data)[0]
                cluster_id = str(kmeans_cluster)

                st.write(f"**An치lisis KMeans:**")
                st.write(f"La empresa pertenece al cl칰ster: **{cluster_id}**")

                if cluster_id in bankrupt_proportions:
                    prop_quiebra_cluster = bankrupt_proportions[cluster_id].get('1', 0)
                    st.info(f"Este cl칰ster (Cl칰ster {cluster_id}) tuvo una proporci칩n de quiebras del **{prop_quiebra_cluster*100:.2f}%** en los datos de entrenamiento.")

                    if cluster_id in cluster_profiles:
                        st.markdown("**Perfil Financiero T칤pico de este Cl칰ster (Valores Escalados):**")
                        profile_df = pd.DataFrame([cluster_profiles[cluster_id]])
                        profile_df.index = [f"Cl칰ster {cluster_id} Media"]

                        actual_display_features = [f for f in DISPLAY_FEATURES_FOR_CLUSTERS if f in profile_df.columns]
                        if actual_display_features:
                            st.dataframe(profile_df[actual_display_features].T.style.format("{:.3f}"))
                            st.markdown("""
                            *(**Interpretaci칩n de valores escalados:** Un valor positivo significa que la empresa en este cl칰ster tiende a tener un valor m치s alto para esta caracter칤stica en comparaci칩n con la media de todas las empresas. Un valor negativo significa que tiende a tener un valor m치s bajo.)*
                            """)
                            cluster_profile_data = {f: profile_df[f].iloc[0] for f in actual_display_features} # Para el LLM
                        else:
                            st.info("No se pudieron mostrar caracter칤sticas clave para este cl칰ster (las caracter칤sticas seleccionadas no coinciden).")
                    else:
                        st.info("No se encontr칩 el perfil de caracter칤sticas para este cl칰ster.")
                else:
                    st.markdown("*(No se encontr칩 informaci칩n de proporciones de quiebra para este cl칰ster. Puede que el archivo JSON est칠 incompleto o no se haya generado.)*")

            except Exception as e:
                st.warning(f"No se pudo aplicar KMeans a los datos de entrada. Error: {e}")
                if pca_transformed_data is not None:
                    st.write("Datos pasados a KMeans (primeros 5 valores de las primeras componentes PCA):", pca_transformed_data[0, :5].tolist())
                    st.write("N칰mero de caracter칤sticas pasadas a KMeans:", pca_transformed_data.shape[1])
                st.stop()
        else:
            st.warning("Modelo KMeans no disponible o PCA fall칩.")


        # --- Secci칩n de An치lisis por IA (Groq) y Chat Interactivo ---
        st.write("---")
        st.subheader("An치lisis de Riesgo Detallado por IA (Groq) y Consultas")

        # Inicializar historial de chat si no existe
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # Bot칩n para generar el an치lisis inicial (solo si no se ha generado ya)
        if not st.session_state.messages:
            if st.button("Generar An치lisis de Riesgo con IA"):
                with st.spinner("Generando an치lisis inicial con IA..."):
                    initial_prompt_content = generate_initial_analysis_prompt_content(
                        prediction, prediction_proba, cluster_id, prop_quiebra_cluster,
                        cluster_profile_data, original_input_df_head
                    )
                    
                    # El primer mensaje al LLM usa el prompt menos restrictivo para el an치lisis
                    messages_for_llm = [
                        {"role": "system", "content": SYSTEM_PROMPT_INITIAL_ANALYSIS}, 
                        {"role": "user", "content": initial_prompt_content}
                    ]
                    
                    ai_response = get_llm_response(messages_for_llm, SYSTEM_PROMPT_INITIAL_ANALYSIS) # Pasa el prompt de sistema aqu칤
                    
                    # A침adir la respuesta inicial de la IA al historial
                    st.session_state.messages.append({"role": "assistant", "content": ai_response})
                    
                    # A침adir una pregunta inicial de la IA para invitar al di치logo
                    st.session_state.messages.append({"role": "assistant", "content": "쯊ienes alguna duda sobre esta empresa, alguna de las m칠tricas o del proyecto en s칤?"})
                    st.rerun() # Recargar para mostrar el chat

        # Mostrar historial de chat
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        # Entrada de chat para nuevas preguntas
        if prompt := st.chat_input("Haz una pregunta a la IA experta en finanzas..."):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            with st.chat_message("assistant"):
                with st.spinner("La IA est치 pensando..."):
                    # Prepara los mensajes para el LLM, incluyendo el contexto del sistema y el historial
                    # Para el chat, usa el prompt m치s restrictivo
                    messages_to_send = st.session_state.messages # El historial ya tiene los roles correctos

                    ai_response = get_llm_response(messages_to_send, SYSTEM_PROMPT_CHAT) # Pasa el prompt de sistema aqu칤
                    st.markdown(ai_response)
                    st.session_state.messages.append({"role": "assistant", "content": ai_response})
        # --- Fin de la secci칩n LLM y Chat Interactivo ---

        st.write("---")
        st.subheader("Guardar Datos de Empresa en Base de Datos")
        if st.button("Guardar esta empresa en la Base de Datos"):
            if not input_df_raw.empty:
                # Convertir el DataFrame a un diccionario para enviar a Flask
                # Tomamos la primera (y 칰nica) fila y la convertimos a dict
                company_data_to_save = input_df_raw.iloc[0].to_dict()
                
                # --- A칌ADIR PREDICCIONES Y CL칔STER AL DICCIONARIO ---
                company_data_to_save['prediction_bankrupt'] = int(prediction) # 0 o 1
                company_data_to_save['prediction_probability'] = float(prediction_proba) # Probabilidad
                company_data_to_save['assigned_cluster_id'] = cluster_id # ID del cl칰ster
                # --- FIN DE A칌ADIR DATOS ---

                try:
                    response = requests.post(FLASK_BACKEND_SAVE_URL, json=company_data_to_save)
                    if response.status_code == 201:
                        st.success(f"Datos de la empresa guardados exitosamente en la base de datos. ID: {response.json().get('id')}")
                    else:
                        st.error(f"Error al guardar datos en la base de datos: {response.status_code} - {response.json().get('error', 'Error desconocido')}")
                except requests.exceptions.ConnectionError:
                    st.error("Error de conexi칩n: Aseg칰rate de que el servidor Flask est칠 corriendo en la URL especificada (ej. http://flask_backend:5000).")
                except Exception as e:
                    st.error(f"Ocurri칩 un error al enviar datos a Flask: {e}")
            else:
                st.warning("No hay datos de empresa cargados para guardar.")


        st.write("---")
        st.subheader("Detalles T칠cnicos (Datos Procesados y Escalados)")
        st.dataframe(df_scaled)


    except Exception as e:
        st.error(f"Ocurri칩 un error general durante el procesamiento o la predicci칩n: {e}")
        st.info("Aseg칰rate de que el archivo CSV tiene el formato correcto, contiene datos num칠ricos v치lidos, y sus nombres de columna son consistentes con tus datos de entrenamiento originales (95 columnas).")

st.markdown("---")
st.info("Para m치s informaci칩n, consulta el c칩digo fuente o contacta al desarrollador.")
